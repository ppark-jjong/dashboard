pip install -r requirement.txt
python -m pip install --upgrade pip
python -m venv venv
venv\Scripts\activate
deactivate


pip install -r requirements.txt


netstat -a -o
taskkill /f /pid PID번호


protoc -I=protos --python_out=protos protos/delivery_status.proto


protoc --proto_path=. -I . --python_out=. --pyi_out=. ./realtime_status.proto
==================================================================


# 기존 컨테이너 중지 및 제거 (선택 사항)
docker-compose down

# 도커 이미지 빌드
docker-compose build

# 도커 컨테이너 실행 (백그라운드 실행)
docker-compose up -d

cd C:\MyMain\test\
docker-compose -f docker/docker-compose.yaml up


# 실행 상태 확인
docker-compose ps

# 로그 실시간 확인 (필요 시)
docker-compose logs -f

docker-compose build --no-cache
docker-compose up

#컨테이너 접근
docker exec -it <container_name> /bin/bash

docker cp test-kafka.py <container_name>:/path/to/new_directory/



# Kafka CLI 명령어를 사용하여 토픽 목록 조회
docker exec -it kafka kafka-topics --list --bootstrap-server localhost:9092


- 토픽 확인법
	컨테이너 내부 진입 만약 잘 안된다면 kafka 설치 경로를 찾아야함
	토픽 검색
	/usr/bin/kafka-topics --bootstrap-server localhost:9092 --delete --topic delivery-data

==================================================================

Google Cloud에 배포 (Cloud  Run 사용 예시)


# 변수 설정
$PROJECT_ID = "dashboard-435121"
$IMAGE_NAME = "fastapi-webhook"

# Docker 명령어에서 변수 사용
docker build -t gcr.io/$PROJECT_ID/$IMAGE_NAME -f docker/Dockerfile.fastapi .
docker push gcr.io/$PROJECT_ID/$IMAGE_NAME

=============================================================

1. 실시간 배송 상태 및 KPI 모니터링
목표: 오늘 날짜의 각 배송 상태별 갯수, 완료율, 평균 배송 시간을 모니터링하여 실시간으로 성과를 확인.

2. 이번주 이슈 모니터링
목표: 이번 주 동안 발생한 이슈 갯수와 관련된 DPS 번호를 추적.

3. 이번주 배송 거리 분석
목표: 이번 주의 평균 배송 거리를 계산하여 분석.

4. 이번달 배송 완료율 트렌드 분석
목표: 주별로 배송 완료율의 변화를 분석하여 성과 추적.

5. 이번달 이슈 발생 패턴 분석
목표: 요일별 이슈 발생 패턴을 분석하여 빈도 파악.

6. 이번달 배송 갯수 트렌드 분석
목표 : SLA 타입별, 요일별 배송 개수 분석



==================================================================

commons-pool2-2.11.1.jar
kafka-clients-3.4.0.jar
spark-protobuf_2.12-3.4.1.jar
spark-sql-kafka-0-10_2.12-3.4.1.jar
spark-token-provider-kafka-0-10_2.12-3.4.1.jar




==================================================================

1. **데이터 소스에서 수집**
    - 기술 스택: Kafka, Python, Flink
    - 데이터가 원시 형태로 수집됩니다.

2. **데이터 레이크 저장**
    - 기술 스택: AWS S3, GCS, HDFS
    - 수집된 원시 데이터가 저장됩니다.

3. **데이터 전처리**
    - 기술 스택: Spark, Flink, Python
    - 데이터를 정제하고 변환하여 처리합니다.

4. **데이터 웨어하우스 저장**
    - 기술 스택: AWS Redshift, BigQuery, Snowflake
    - 전처리된 데이터가 구조화된 형태로 저장됩니다.

5. **데이터 마트로 분석 제공**
    - 기술 스택: SQL, OLAP Tools
    - 비즈니스 요구에 맞게 데이터를 추출하여 분석에 활용합니다.

6. **데이터 자동화**
    - 기술 스택: Airflow, Lambda
    - 파이프라인 자동화를 통해 스케줄링 및 트리거 실행을 관리합니다.

7. **클라우드 인프라 활용**
    - 기술 스택: AWS, GCP, Azure
    - 클라우드 기반에서 데이터 저장 및 처리, 자동화가 이루어집니다.



==================================================================

main/
├── src/
│   ├── api/                           # API 관련 코드
│   │   ├── webhook_api.py             # Webhook을 통한 데이터 수집 (FastAPI)
│   │
│   ├── brokers/                       # 메시지 브로커 관련 코드
│   │   ├── kafka/
│   │       ├── producer.py            # Kafka Producer - 데이터 전송
│   │       ├── consumer.py            # Kafka Consumer - PySpark와 연동
│   │
│   ├── collectors/                    # 데이터 수집 관련 코드
│   │   ├── google_sheets.py           # Google Sheets 데이터 수집
│   │   ├── web_crawler.py             # 웹 크롤러를 통한 데이터 수집
│   │
│   ├── config/                        # 설정 관련 코드
│   │   ├── config_data.py             # Kafka, Spark 설정
│   │   ├── config_google.py           # GCS, Google Sheets 설정
│   │   ├── config_web_crawler.py      # Web Crawler 설정
│   │   ├── config_manager.py          # 전체 설정 관리 파일
│   │
│   ├── processors/                    # 데이터 처리 관련 코드
│   │   ├── realtime_data_processor.py # 실시간 데이터 처리 로직
│   │   ├── historical_data_processor.py # 과거 데이터 처리 로직
│   │
│   ├── utils/                         # 유틸리티 함수 모음
│   │   ├── file_handler.py            # 파일 관리 (다운로드, 업로드 등)
│   │   ├── data_transformer.py        # 데이터 형식 변환 함수
│
├── test/                              # 테스트 코드
│   ├── test_collectors.py             # 데이터 수집 관련 테스트 코드
│   ├── test_processors.py             # 데이터 처리 관련 테스트 코드
│   ├── test_api.py                    # API 관련 테스트 코드
│
├── docker/
│   ├── Dockerfile                     # 도커 이미지 빌드 설정
│   ├── docker-compose.yaml            # 도커 컴포즈 설정 (Kafka, PySpark, Zookeeper)
│
├── oauth/                           # 민감한 정보 (API 인증 정보 등)
│   └── google/                        # Google API 인증서 파일
│       └── credentials.json           # Google API 인증서
│
└── requirements.txt                   # 프로젝트 의존성 관리
